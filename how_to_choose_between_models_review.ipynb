{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TYPES OF MODELS\n",
    "\n",
    "[REFERENCE: WHAT ARE THE ADVANTAGES OF DIFFERENT CLASSIFICATION ALGORITHMS](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms)\n",
    "\n",
    "\n",
    "### PARAMETRIC vs NON-PARAMETRIC MACHINE LEARNING ALGORITHMS\n",
    "\n",
    "Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.\n",
    "\n",
    "The algorithms involve two steps:\n",
    "\n",
    "Select a form for the function.\n",
    "Learn the coefficients for the function from the training data.\n",
    "Some examples of parametric machine learning algorithms are Linear Regression and Logistic Regression.\n",
    "\n",
    "Algorithms that do not make strong assumptions about the form of the mapping function are called __nonparametric__ machine learning algorithms. By not making assumptions, they are free to learn any functional form from the training data.\n",
    "\n",
    "Non-parametric methods are often more flexible, achieve better accuracy but require a lot more data and training time.\n",
    "\n",
    "Examples of nonparametric algorithms include Support Vector Machines, Neural Networks and Decision Trees.\n",
    "\n",
    "\n",
    "### BIAS-VARIANCE TRADEOFF\n",
    "\n",
    "Machine learning algorithms can best be understood through the lens of the bias-variance trade-off.\n",
    "\n",
    "Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "\n",
    "Generally parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.\n",
    "\n",
    "Decision trees are an example of a low bias algorithm, whereas linear regression is an example of a high-bias algorithm.\n",
    "\n",
    "Variance is the amount that the estimate of the target function will change if different training data was used. The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance, not zero variance.\n",
    "\n",
    "The k-Nearest Neighbors algorithm is an example of a high-variance algorithm, whereas Linear Discriminant Analysis is an example of a low variance algorithm.\n",
    "\n",
    "The goal of any predictive modeling machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance. The parameterization of machine learning algorithms is often a battle to balance out bias and variance.\n",
    "\n",
    "Increasing the bias will decrease the variance.\n",
    "Increasing the variance will decrease the bias.\n",
    "\n",
    "### LINEAR REGRESSION\n",
    "\n",
    "Linear regression is perhaps one of the most well known and well understood algorithms in statistics and machine learning.\n",
    "\n",
    "#### Isn't it a technique from statistics?\n",
    "\n",
    "Predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. We will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.\n",
    "\n",
    "The representation of linear regression is a equation that describes a line that best fits the relationship between the input variables (x) and the output variables (y), by finding specific weightings for the input variables called coefficients (B).\n",
    "\n",
    "For example:\n",
    "\n",
    "y = B0 + B1 * x\n",
    "\n",
    "We will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients B0 and B1.\n",
    "\n",
    "Different techniques can be used to learn the linear regression model from data, such as a linear algebra solution for ordinary least squares and gradient descent optimization.\n",
    "\n",
    "Linear regression has been around for more than 200 years and has been extensively studied. Some good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data, if possible.\n",
    "\n",
    "It is a fast and simple technique and good first algorithm to try.\n",
    "\n",
    "\n",
    "### LOGISTIC REGRESSION\n",
    "\n",
    "Logistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).\n",
    "\n",
    "Logistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable.\n",
    "\n",
    "Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.\n",
    "\n",
    "The logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. If less than 0.5 then output 1) and predict a class value.\n",
    "\n",
    "Because of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. This can be useful on problems where you need to give more rationale for a prediction.\n",
    "\n",
    "Like linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other.\n",
    "\n",
    "It's a fast model to learn and effective on binary classification problems.\n",
    "\n",
    "### LINEAR DISCRIMINANT ANALYSIS\n",
    "\n",
    "Logistic Regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then the Linear Discriminant Analysis algorithm is the preferred linear classification technique.\n",
    "The representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class. For a single input variable this includes:\n",
    "\n",
    "- The mean value for each class\n",
    "- The variance calculated across all classes\n",
    "- Predictions are made by calculating a discriminate value for each class and making a prediction for the class with the largest value\n",
    "\n",
    "The technique assumes that the data has a Gaussian distribution (bell curve), so it is a good idea to remove outliers from your data before hand.\n",
    "\n",
    "It's a simple and powerful method for classification predictive modeling problems.\n",
    "\n",
    "### DECISION TREES\n",
    "\n",
    "Decision Trees are an important type of algorithm for predictive modeling machine learning.\n",
    "\n",
    "The representation for the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n",
    "\n",
    "The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.  Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "Trees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.\n",
    "\n",
    "Decision trees have a high variance and can yield more accurate predictions when used in an ensemble.\n",
    "\n",
    "### NAIVE BAYES\n",
    "\n",
    "Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.\n",
    "\n",
    "The model is comprised of two types of probabilities that can be calculated directly from your training data:\n",
    "\n",
    "- The probability of each class\n",
    "- The conditional probability for each class given each x value\n",
    "\n",
    "Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem.\n",
    "\n",
    "When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.\n",
    "\n",
    "Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.\n",
    "\n",
    "### K-NEAREST NEIGHBORS\n",
    "\n",
    "The KNN algorithm is very simple and very effective.\n",
    "\n",
    "The model representation for KNN is the entire training dataset. Simple right?\n",
    "\n",
    "Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.\n",
    "\n",
    "The trick is in how to determine similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance, a number you can calculate directly based on the differences between each input variable.\n",
    "\n",
    "KNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed, just in time. You can also update and curate your training instances over time to keep predictions accurate.\n",
    "\n",
    "The idea of distance or closeness can break down in very high dimensions (lots of input variables) which can negatively effect the performance of the algorithm on your problem. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.\n",
    "\n",
    "A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset.\n",
    "\n",
    "### LEARNING VECTOR QUANTIZATION\n",
    "\n",
    "The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.\n",
    "\n",
    "> __Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression.__\n",
    "\n",
    "The representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm.\n",
    "\n",
    "After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction.\n",
    "\n",
    "Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.\n",
    "\n",
    "If you discover that KNN gives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset.\n",
    "\n",
    "### SUPPORT VECTOR MACHINES\n",
    "\n",
    "Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.\n",
    "\n",
    "A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1.\n",
    "\n",
    "In two-dimensions you can visualize this as a line and let's assume that all of our input points can be completely separated by this line.\n",
    "\n",
    "The SVM learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane.\n",
    "\n",
    "The distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that as the largest margin.\n",
    "\n",
    "Only these points are relevant in defining the hyperplane and in the construction of the classifier.\n",
    "\n",
    "These points are called the support vectors. They support or define the hyperplane.\n",
    "\n",
    "In practice, an optimization algorithm is used to find the values for the coefficients that maximizes the margin.\n",
    "\n",
    "SVM might be one of the most powerful out-of-the-box classifiers and worth trying on your dataset.\n",
    "\n",
    "### RANDOM FOREST\n",
    "\n",
    "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.\n",
    "\n",
    "The bootstrap is a powerful statistical method for estimating a quantity from a data sample. Such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value.\n",
    "\n",
    "In bagging, the same approach is used, but instead for estimating entire statistical models, most commonly decision trees.\n",
    "\n",
    "Multiple samples of your training data are taken then models are constructed for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n",
    "\n",
    "Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.\n",
    "\n",
    "The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n",
    "\n",
    "If you get good good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.\n",
    "\n",
    "\n",
    "### BOOSTING\n",
    "\n",
    "Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers.\n",
    "\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "\n",
    "AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.\n",
    "\n",
    "AdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more more weight, whereas easy to predict instances are given less weight.\n",
    "\n",
    "Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence.\n",
    "\n",
    "After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on the training data.\n",
    "\n",
    "Because so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO CHOOSE BETWEEN LOGISTIC REGRESSION, DECISION TREES AND SUPPORT VECTOR MACHINES\n",
    "\n",
    "All three of these techniques have certain properties inherent by their design, we’ll elaborate on some in order to provide you with few pointers on their selection for your particular business problem.\n",
    "\n",
    "### LOGISTIC REGRESSION\n",
    "\n",
    "We’ll start with Logistic Regression, the most prevalent algorithm for solving industry scale problems, although its losing ground to other techniques with progress in efficiency and implementation ease of other complex algorithms.\n",
    "\n",
    "A very convenient and useful side effect of a logistic regression solution is that it doesn’t give you discrete output or outright classes as output. Instead you get probabilities associated with each observation. You can apply many standard and custom performance metrics on this probability score to get a cutoff and in turn classify output in a way which best fits your business problem. \n",
    "\n",
    "> __A very popular application of this property is scorecards in the financial industry, where you can adjust your threshold [cutoff] to get different results for classification from the same model.__ \n",
    " \n",
    "Very few other algorithms provide such scores as a direct result. Instead their outputs are discreet direct classifications. Also, logistic regression is pretty efficient in terms of time and memory requirement. It can be applied on distributed data and it also has online algorithm implementation to handle large data on less resources.\n",
    "\n",
    "In addition to above, logistic regression algorithm is -\n",
    "\n",
    "- Robust to small noise in the data   \n",
    "- Not particularly affected by mild cases of multi-collinearity\n",
    "- Severe cases of multi-collinearity can be handled by implementing logistic regression with L2 regularization. \n",
    "\n",
    "If a parsimonious model is needed, L2 regularization is not the best choice because it keeps all the features in the model.\n",
    "\n",
    "> __A parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible. For model evaluation, there are different methods depending on what you want to know.__\n",
    "\n",
    "Where logistic regression starts to falter is when you have a large number of features and good chunk of missing data. Too many categorical variables are also a problem for logistic regression. Another criticism of logistic regression can be that it uses the entire data for coming up with its scores. Although this is not a problem as such, but it can be argued that “obvious” cases which lie at the extreme end of scores should not really be a concern when you are trying to come up with a separation curve. It should ideally be dependent on those boundary cases, some might argue. Also if some of the features are non-linear, you’ll have to rely on transformations, which become a hassle as size of your feature space increases. We have picked few prominent pros and cons from our discussion to summaries things for logistic regression.\n",
    "\n",
    "#### LOGISTIC REGRESSION PROS:\n",
    "\n",
    "1. Convenient probability scores for observations\n",
    "2. Efficient implementations available across tools\n",
    "3. Multi-collinearity is not really an issue and can be countered with L2 regularization to an extent.\n",
    "4. Wide spread industry comfort for logistic regression solutions \n",
    "\n",
    "#### LOGISTIC REGRESSION CONS:\n",
    "\n",
    "1. Doesn’t perform well when feature space is too large\n",
    "2. Doesn’t handle large number of categorical features/variables well\n",
    "3. Relies on transformations for non-linear features\n",
    "4. Relies on entire data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREES\n",
    "\n",
    "Decision trees are inherently indifferent to monotonic transformation or non-linear features [this is different from non-linear correlation among predictors] because they simply cut feature space in rectangles [or (hyper) cuboids] which can adjust themselves to any monotonic transformation. Since decision trees anyway are designed to work with discrete intervals or classes of predictors, any number of categorical variables are not really an issue with decision trees. Models obtained from decision tree are fairly intuitive and easy to explain to business. Probability scores are not a direct result but you can use class probabilities assigned to terminal nodes instead. This brings us to the biggest problem associated with Decision Trees, that is, they are highly biased class of models. You can make a decision tree model on your training set which might outperform all other algorithms but it’ll prove to be a poor predictor on your test set. You’ll have to rely heavily on pruning and cross validation to get a non-over-fitting model with Decision Trees.\n",
    "\n",
    "> __A monotonic transformation is a way of transforming one set of numbers into another set of numbers in a way that preserves the order of the numbers.__\n",
    "\n",
    "This problem of over-fitting is overcome to large extent by using Random Forests, which are nothing but a very clever extension of decision trees. But random forest take away easy to explain business rules because now you have thousands of such trees and their majority votes to make things complex. Also by design, decision trees have forced interactions between variables, which makes them rather inefficient if most of your variables have no or very weak interactions. On the other hand this design also makes them rather less susceptible to multi-collinearity. \n",
    "\n",
    "#### DECISION TREES PROS:\n",
    "\n",
    "1. Intuitive Decision Rules\n",
    "2. Can handle non-linear features\n",
    "3. Take into account variable interactions\n",
    "\n",
    "#### DECISION TREES CONS:\n",
    "\n",
    "1. Highly biased to training set [Random Forests to your rescue]\n",
    "2. No ranking score as direct result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINES\n",
    "\n",
    "Now to Support Vector Machines. The best thing about support vector machines is that they rely on boundary cases to build the much needed separating curve. They can handle non-linear decision boundaries. Reliance on boundary cases also enables them to handle missing data for “obvious” cases. SVM can handle large feature spaces which makes them one of the favorite algorithms in text analysis which almost always results in huge number of features where logistic regression is not a very good choice.\n",
    "\n",
    "Result of SVMs are not as not as intuitive as decision trees for a layman. With non-linear kernels, SVMs can be very costly to train on huge data. In Summary:\n",
    " \n",
    "#### SVM PROS:\n",
    "\n",
    "1. Can handle large feature space\n",
    "2. Can handle non-linear feature interactions\n",
    "3. Do not rely on entire data\n",
    "\n",
    "#### SVM CONS:\n",
    "\n",
    "1. Not very efficient with large number of observations\n",
    "2. It can be tricky to find appropriate kernel sometimes\n",
    "\n",
    "Here is a simple workflow for you to decide which algorithm to use out of these three -\n",
    "\n",
    "- Always start with logistic regression, if nothing then to use the performance as baseline\n",
    "- See if decision trees (Random Forests) provide significant improvement. Even if you do not end up using the resultant model, you can use random forest results to remove noisy variables.\n",
    "- Go for SVM if you have large number of features and number of observations are not a limitation for available resources and time\n",
    "\n",
    "At the end of the day, remember that good data beats any algorithm anytime. Always see if you can engineer a good feature by using your domain knowledge. Try various iterations of your ideas while experimenting with feature creation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLES\n",
    "\n",
    "Another thing to try with efficient computing infrastructure available these days is to use ensembles of multiple models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
